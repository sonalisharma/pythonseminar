{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Browing through all images from the folders and extracting features "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "AY 250 - Scientific Research Computing with Python\n",
      "Homework Assignment 4 - Parallel Feature Extraction Example\n",
      "Author: Christopher Klein, Joshua Bloom\n",
      "\"\"\"\n",
      "from os import listdir\n",
      "import os \n",
      "from multiprocessing import Pool, cpu_count\n",
      "from pylab import imread\n",
      "from time import time\n",
      "import scipy as sp\n",
      "import numpy as np\n",
      "from skimage.filter import sobel\n",
      "\n",
      "## CHANGE THIS NEXT LINE!\n",
      "MYDIRECTORY = MYDIRECTORY =\"/Users/sonali/Documents/Ischool/PythonSeminar/pythonseminar/HW4/50_categories\"\n",
      "\n",
      "# FUNCTION DEFINITIONS\n",
      "# Quick function to divide up a large list into multiple small lists, \n",
      "# attempting to keep them all the same size. \n",
      "def split_seq(seq, size):\n",
      "        newseq = []\n",
      "        splitsize = 1.0/size*len(seq)\n",
      "        for i in range(size):\n",
      "            newseq.append(seq[int(round(i*splitsize)):\n",
      "                int(round((i+1)*splitsize))])\n",
      "        return newseq\n",
      "# Our simple feature extraction function. It takes in a list of image paths, \n",
      "# does some measurement on each image, then returns a list of the image paths\n",
      "# paired with the results of the feature measurement.\n",
      "def extract_features(image_path_list):\n",
      "    feature_list = []\n",
      "    for image_path in image_path_list:\n",
      "        image_array = imread(image_path)\n",
      "        img_size = image_array.size\n",
      "        red_channel_mean= image_array[...,0].mean()\n",
      "        green_channel_mean= image_array[...,1].mean()\n",
      "        blue_channel_mean= image_array[...,2].mean()\n",
      "        red_channel_sd= image_array[...,0].std()\n",
      "        green_channel_sd= image_array[...,1].std()\n",
      "        blue_channel_sd= image_array[...,2].std()\n",
      "        try:\n",
      "            r, g, b = image_array[:,:,0], image_array[:,:,1], image_array[:,:,2]\n",
      "            imgarray1_gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
      "        except:\n",
      "            pass\n",
      "        max_gray_x_loc = np.argwhere(imgarray1_gray.max() == imgarray1_gray)[...,0].mean()\n",
      "        max_gray_y_loc = np.argwhere(imgarray1_gray.max() == imgarray1_gray)[...,1].mean()\n",
      "        min_gray_x_loc = np.argwhere(imgarray1_gray.min() == imgarray1_gray)[...,0].mean()\n",
      "        min_gray_y_loc = np.argwhere(imgarray1_gray.min() == imgarray1_gray)[...,1].mean()\n",
      "        min_gray_x_loc_std = np.argwhere(imgarray1_gray.min() == imgarray1_gray)[...,0].std()\n",
      "        min_gray_y_loc_std = np.argwhere(imgarray1_gray.min() == imgarray1_gray)[...,1].std()\n",
      "        max_gray_x_loc_std = np.argwhere(imgarray1_gray.max() == imgarray1_gray)[...,0].std()\n",
      "        max_gray_y_loc_std = np.argwhere(imgarray1_gray.max() == imgarray1_gray)[...,1].std()\n",
      "        \n",
      "        imgarray1_gray = np.array(imgarray1_gray, dtype=np.float64)\n",
      "        edges = sobel(imgarray1_gray)\n",
      "        edges_height = edges.shape[0]\n",
      "        edges_width = edges.shape[1]\n",
      "      \n",
      "        feature_list.append([image_path.split(\"/\")[-2],image_path.split(\"/\")[-1], \n",
      "                             img_size,\n",
      "                             red_channel_mean,\n",
      "                             green_channel_mean,\n",
      "                             blue_channel_mean,\n",
      "                             red_channel_sd,\n",
      "                             green_channel_sd,\n",
      "                             blue_channel_sd,\n",
      "                             max_gray_x_loc,\n",
      "                             max_gray_y_loc,\n",
      "                             min_gray_x_loc,\n",
      "                             min_gray_y_loc,\n",
      "                             max_gray_x_loc_std,\n",
      "                             max_gray_y_loc_std,\n",
      "                             min_gray_x_loc_std,\n",
      "                             min_gray_y_loc_std,\n",
      "                             edges_height,\n",
      "                             edges_width                              \n",
      "                             ])\n",
      "    return feature_list\n",
      "\n",
      "\n",
      "\n",
      "### Main program starts here ###################################################\n",
      "# We first collect all the local paths to all the images in one list\n",
      "image_paths = []\n",
      "categories = listdir(MYDIRECTORY)\n",
      "for category in categories:\n",
      "    image_names = listdir(MYDIRECTORY  + \"/\" + category)\n",
      "    for name in image_names:\n",
      "        image_paths.append(MYDIRECTORY + \"/\" + category + \"/\" + name)\n",
      "\n",
      "print (\"There should be 4244 images, actual number is \" + \n",
      "    str(len(image_paths)) + \".\")\n",
      "\n",
      "# Then, we run the feature extraction function using multiprocessing.Pool so \n",
      "# so that we can parallelize the process and run it much faster.\n",
      "numprocessors = cpu_count() # To see results of parallelizing, set numprocessors\n",
      "                            # to less than cpu_count().\n",
      "# numprocessors = 1\n",
      "\n",
      "# We have to cut up the image_paths list into the number of processes we want to\n",
      "# run. \n",
      "split_image_paths = split_seq(image_paths, numprocessors)\n",
      "\n",
      "# Ok, this block is where the parallel code runs. We time it so we can get a \n",
      "# feel for the speed up.\n",
      "start_time = time()\n",
      "p = Pool(numprocessors)\n",
      "result = p.map_async(extract_features, split_image_paths)\n",
      "poolresult = result.get()\n",
      "end_time = time()\n",
      "\n",
      "# All done, print timing results.\n",
      "print (\"Finished extracting features. Total time: \" + \n",
      "    str(round(end_time-start_time, 3)) + \" s, or \" + \n",
      "    str( round( (end_time-start_time)/len(image_paths), 5 ) ) + \" s/image.\")\n",
      "# This took about 10-11 seconds on my 2.2 GHz, Core i7 MacBook Pro. It may also\n",
      "# be affected by hard disk read speeds.\n",
      "\n",
      "# To tidy-up a bit, we loop through the poolresult to create a final list of\n",
      "# the feature extraction results for all images.\n",
      "combined_result = []\n",
      "for single_proc_result in poolresult:\n",
      "    for single_image_result in single_proc_result:\n",
      "        combined_result.append(single_image_result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There should be 4244 images, actual number is 4244.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished extracting features. Total time: 154.403 s, or 0.03638 s/image.\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Creating test and training sets and normalizng features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import preprocessing\n",
      "import math\n",
      "np.random.shuffle(combined_result)\n",
      "X = [c[2:] for c in combined_result]\n",
      "Y = np.array([c[0] for c in combined_result],dtype=\"string\")\n",
      "X_scaled = preprocessing.scale(X)\n",
      "split = int(math.floor(len(combined_result)* 0.9))\n",
      "split\n",
      "train_X = X_scaled[:split]\n",
      "train_Y = Y[:split]\n",
      "test_X = X_scaled[split:]\n",
      "test_Y = Y[split:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Creating model using Random Forest Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "def fitrf(Xtr,Ytr,ntree,mtry,Xte,Yte):\n",
      "    clf = RandomForestClassifier(n_estimators=ntree,max_features=mtry)\n",
      "    clf.fit(Xtr, Ytr)\n",
      "    preds = clf.predict(Xte)\n",
      "    print float(sum(preds!=Yte))/len(Yte)\n",
      "    return clf\n",
      "model = fitrf(train_X,train_Y,100,17,test_X,test_Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.684705882353\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Calculating accuracy using 20 fold cross validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn import cross_validation\n",
      "\n",
      "def print_cv_score_summary(model, xx, yy, cv):\n",
      "    scores = cross_val_score(model, xx, yy, cv=cv, n_jobs=1) #n_jobs = -1 runs it in parallel\n",
      "    print(\"mean: {:3f}, stdev: {:3f}\".format(\n",
      "        np.mean(scores), np.std(scores)))\n",
      "    \n",
      "print_cv_score_summary(model,X,Y,cv=cross_validation.KFold(len(Y), 20))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "mean: 0.292187, stdev: 0.029380\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Saving the model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "pickle.dump(model, open('trained_classifier.p','w') )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}